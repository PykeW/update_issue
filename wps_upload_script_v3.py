#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
WPSè¡¨æ ¼ç®€å•ä¸Šä¼ è„šæœ¬ - ä¼˜åŒ–ç‰ˆ
ç›´æ¥ä»WPSè¡¨æ ¼è¯»å–æ•°æ®å¹¶ä¸Šä¼ åˆ°æœåŠ¡å™¨
"""

import requests
import json
import pandas as pd
import logging
import time
from datetime import datetime
from functools import wraps

# ==================== é…ç½®ä¿¡æ¯ ====================
CONFIG = {
    'server': {
        'base_url': 'http://114.55.118.105',
        'upload_endpoint': '/api/wps/upload',
        'timeout': 30
    },
    'wps': {
        'range': '$B:$P',
        'sheet_name': 'é—®é¢˜æ¸…å•',
        'target_category': 'è½¯ä»¶'
    },
    'upload': {
        'batch_size': 50,
        'max_retries': 3,
        'retry_delay': 1
    },
    'fields': {
        'required': ['é¡¹ç›®åç§°', 'é—®é¢˜åˆ†ç±»'],
        'mapping': {
            'åºå·': 'serial_number',
            'é¡¹ç›®åç§°': 'project_name',
            'é—®é¢˜åˆ†ç±»': 'problem_category',
            'ä¸¥é‡ç¨‹åº¦': 'severity_level',
            'é—®é¢˜/éœ€æ±‚æè¿°': 'problem_description',
            'è§£å†³æ–¹æ¡ˆ': 'solution',
            'è¡ŒåŠ¨ä¼˜å…ˆçº§': 'action_priority',
            'è¡ŒåŠ¨è®°å½•': 'action_record',
            'å‘èµ·äºº': 'initiator',
            'è´£ä»»äºº': 'responsible_person',
            'çŠ¶æ€': 'status',
            'å¼€å§‹æ—¶é—´': 'start_time',
            'ç›®æ ‡å®Œæˆæ—¶é—´': 'target_completion_time',
            'å®å®Œæ—¶é—´': 'actual_completion_time',
            'å¤‡æ³¨': 'remarks'
        }
    }
}

# æœåŠ¡å™¨URLé…ç½®
SERVER_URL = f"{CONFIG['server']['base_url']}{CONFIG['server']['upload_endpoint']}"

# ==================== å·¥å…·å‡½æ•° ====================
def setup_logging():
    """è®¾ç½®æ—¥å¿— - WPSç¯å¢ƒé€‚é…ç‰ˆ"""
    # åˆ›å»ºlogger
    logger = logging.getLogger('WPSUploader')
    logger.setLevel(logging.INFO)

    # é¿å…é‡å¤æ·»åŠ handler
    if not logger.handlers:
        # åˆ›å»ºformatter
        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

        # åªä½¿ç”¨æ§åˆ¶å°handlerï¼ˆWPSç¯å¢ƒä¸å…è®¸æ–‡ä»¶å†™å…¥ï¼‰
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        logger.addHandler(console_handler)

    return logger

def retry(max_attempts=3, delay=1):
    """é‡è¯•è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts - 1:
                        raise e
                    print(f"âš ï¸ ç¬¬ {attempt + 1} æ¬¡å°è¯•å¤±è´¥: {e}")
                    time.sleep(delay * (2 ** attempt))  # æŒ‡æ•°é€€é¿
            return None
        return wrapper
    return decorator

def is_empty_value(value):
    """åˆ¤æ–­å€¼æ˜¯å¦ä¸ºç©º"""
    if pd.isna(value):
        return True
    str_value = str(value).strip().lower()
    return str_value in ['', 'nan', 'none', 'null']

def safe_convert_int(value, default=0):
    """å®‰å…¨è½¬æ¢ä¸ºæ•´æ•°"""
    try:
        if is_empty_value(value):
            return default
        return int(float(str(value)))
    except (ValueError, TypeError):
        return default

def clean_string_value(value):
    """æ¸…ç†å­—ç¬¦ä¸²å€¼"""
    if is_empty_value(value):
        return ''
    return str(value).strip()

# ==================== æ•°æ®å¤„ç†å‡½æ•° ====================
def clean_and_validate_data(df):
    """æ¸…æ´—å’ŒéªŒè¯æ•°æ®"""
    print("ğŸ§¹ å¼€å§‹æ¸…æ´—æ•°æ®...")

    # ç§»é™¤å®Œå…¨ç©ºçš„è¡Œ
    initial_rows = len(df)
    df = df.dropna(how='all').reset_index(drop=True)
    print(f"ğŸ“‹ ç§»é™¤ç©ºè¡Œ: {initial_rows} -> {len(df)}")

    # æ¸…ç†å­—ç¬¦ä¸²åˆ—ï¼Œå¤„ç†nanå€¼
    for col in df.columns:
        if df[col].dtype == 'object':
            df[col] = df[col].fillna('').astype(str)
            # å¤„ç†å„ç§ç©ºå€¼è¡¨ç¤º
            df[col] = df[col].replace(['nan', 'NaN', 'null', 'None', 'NULL'], '')
            df[col] = df[col].str.strip()

    print("âœ… æ•°æ®æ¸…æ´—å®Œæˆ")
    return df

def find_header_row(df, target_columns=['é—®é¢˜åˆ†ç±»', 'é¡¹ç›®åç§°']):
    """æ™ºèƒ½æŸ¥æ‰¾è¡¨å¤´è¡Œ"""
    print("ğŸ” æ™ºèƒ½æŸ¥æ‰¾è¡¨å¤´è¡Œ...")

    for idx, row in df.iterrows():
        row_values = [clean_string_value(val) for val in row.values if not is_empty_value(val)]

        # æ£€æŸ¥æ˜¯å¦åŒ…å«ç›®æ ‡åˆ—å
        matches = sum(1 for col in target_columns if any(col in val for val in row_values))
        match_ratio = matches / len(target_columns)

        if match_ratio >= 0.7:  # 70%åŒ¹é…åº¦
            print(f"âœ… æ‰¾åˆ°è¡¨å¤´è¡Œ: ç¬¬ {idx+1} è¡Œ (åŒ¹é…åº¦: {match_ratio:.1%})")
            return idx

    print("âš ï¸ æœªæ‰¾åˆ°æ ‡å‡†è¡¨å¤´è¡Œ")
    return None

def standardize_column_names(df, header_row):
    """æ ‡å‡†åŒ–åˆ—å"""
    new_columns = []
    for i, val in enumerate(df.iloc[header_row].values):
        col_name = clean_string_value(val) if not is_empty_value(val) else f"Unnamed_{i}"
        new_columns.append(col_name)

    print(f"ğŸ“‹ æ ‡å‡†åŒ–åˆ—å: {new_columns}")

    df.columns = new_columns
    result_df = df.iloc[header_row+1:].reset_index(drop=True)

    print(f"ğŸ“Š å¤„ç†åæ•°æ®è¡Œæ•°: {len(result_df)}")
    return result_df

def is_valid_software_record(problem_category):
    """åˆ¤æ–­æ˜¯å¦ä¸ºæœ‰æ•ˆçš„è½¯ä»¶è®°å½•"""
    if is_empty_value(problem_category):
        return False
    category_str = clean_string_value(problem_category)
    return CONFIG['wps']['target_category'] in category_str

def generate_issue_title(project_name, problem_description):
    """ç”Ÿæˆè®®é¢˜æ ‡é¢˜"""
    project_name = clean_string_value(project_name)
    problem_description = clean_string_value(problem_description)

    if project_name and problem_description:
        return f"{project_name} - {problem_description}"
    elif project_name:
        return project_name
    elif problem_description:
        return problem_description
    else:
        return "æœªå‘½åè®®é¢˜"

def validate_required_fields(record, required_fields):
    """éªŒè¯å¿…å¡«å­—æ®µ"""
    missing = []
    for field in required_fields:
        field_value = record.get(field, '')
        if is_empty_value(field_value):
            missing.append(field)
    return missing

def transform_record(row_data, field_mapping):
    """è½¬æ¢å•æ¡è®°å½•"""
    record = {}

    # è½¬æ¢å­—æ®µ
    for source_field, target_field in field_mapping.items():
        value = row_data.get(source_field, '')

        # ç‰¹æ®Šå¤„ç†æ•°å­—å­—æ®µ
        if target_field in ['severity_level', 'action_priority']:
            record[target_field] = safe_convert_int(value)
        else:
            record[target_field] = clean_string_value(value)

    # ç”Ÿæˆè®®é¢˜æ ‡é¢˜
    project_name = record.get('project_name', '')
    problem_description = record.get('problem_description', '')
    record['issue_title'] = generate_issue_title(project_name, problem_description)

    return record

# ==================== ç½‘ç»œè¯·æ±‚å‡½æ•° ====================
@retry(max_attempts=3)
def test_server_connection():
    """æµ‹è¯•æœåŠ¡å™¨è¿æ¥"""
    print("ğŸŒ æµ‹è¯•æœåŠ¡å™¨è¿æ¥...")

    response = requests.get(
        CONFIG['server']['base_url'],
        timeout=CONFIG['server']['timeout']
    )

    if response.status_code == 200:
        print("âœ… æœåŠ¡å™¨è¿æ¥æˆåŠŸ")
        return True
    else:
        print(f"âŒ æœåŠ¡å™¨è¿æ¥å¤±è´¥: HTTP {response.status_code}")
        return False

@retry(max_attempts=3, delay=1)
def upload_batch(batch_data, batch_num, total_batches):
    """ä¸Šä¼ å•æ‰¹æ•°æ®"""
    print(f"ğŸ“¤ ä¸Šä¼ ç¬¬ {batch_num}/{total_batches} æ‰¹æ•°æ® ({len(batch_data)} æ¡è®°å½•)...")

    upload_payload = {
        'table_data': batch_data,
        'client_info': {
            'version': '2.0.0',
            'timestamp': datetime.now().isoformat(),
            'batch_info': {
                'current_batch': batch_num,
                'total_batches': total_batches,
                'batch_size': len(batch_data)
            },
            'source': 'WPSè¡¨æ ¼-ä¼˜åŒ–ç‰ˆ'
        }
    }

    response = requests.post(
        SERVER_URL,
        json=upload_payload,
        headers={'Content-Type': 'application/json'},
        timeout=CONFIG['server']['timeout']
    )

    return response

def upload_data_in_batches(api_data):
    """åˆ†æ‰¹ä¸Šä¼ æ•°æ®"""
    batch_size = CONFIG['upload']['batch_size']
    total_batches = (len(api_data) + batch_size - 1) // batch_size
    successful_uploads = 0
    failed_uploads = 0

    print(f"ğŸš€ å¼€å§‹åˆ†æ‰¹ä¸Šä¼ : æ€»è®¡ {len(api_data)} æ¡è®°å½•, åˆ† {total_batches} æ‰¹")

    for i in range(0, len(api_data), batch_size):
        batch = api_data[i:i + batch_size]
        batch_num = (i // batch_size) + 1

        try:
            response = upload_batch(batch, batch_num, total_batches)
            if response.status_code == 200:
                result = response.json()
                if result.get('success'):
                    successful_uploads += len(batch)
                    print(f"âœ… ç¬¬ {batch_num} æ‰¹ä¸Šä¼ æˆåŠŸ")
                else:
                    failed_uploads += len(batch)
                    print(f"âŒ ç¬¬ {batch_num} æ‰¹ä¸šåŠ¡å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}")
            else:
                failed_uploads += len(batch)
                print(f"âŒ ç¬¬ {batch_num} æ‰¹ä¸Šä¼ å¤±è´¥: HTTP {response.status_code}")
                try:
                    error_text = response.text[:200] if response.text else "æ— å“åº”å†…å®¹"
                    print(f"   å“åº”å†…å®¹: {error_text}")
                except:
                    pass
        except Exception as e:
            failed_uploads += len(batch)
            print(f"âŒ ç¬¬ {batch_num} æ‰¹ä¸Šä¼ å¼‚å¸¸: {e}")

    print(f"ğŸ“Š ä¸Šä¼ å®Œæˆ: æˆåŠŸ {successful_uploads}, å¤±è´¥ {failed_uploads}")
    return successful_uploads

# ==================== ä¸»è¦ä¸šåŠ¡å‡½æ•° ====================
def read_and_process_wps_data():
    """è¯»å–å’Œå¤„ç†WPSæ•°æ®"""
    try:
        # è¯»å–WPSè¡¨æ ¼æ•°æ®
        print("ğŸ“– è¯»å–WPSè¡¨æ ¼æ•°æ®...")
        df = xl(CONFIG['wps']['range'], headers=True, sheet_name=CONFIG['wps']['sheet_name'])

        if df is None or df.empty:
            print("âŒ æ²¡æœ‰è¯»å–åˆ°æ•°æ®")
            return None

        print(f"âœ… è¯»å–åˆ° {len(df)} è¡ŒåŸå§‹æ•°æ®")

        # æ¸…æ´—æ•°æ®
        df = clean_and_validate_data(df)

        # æŸ¥æ‰¾è¡¨å¤´è¡Œ
        header_row = find_header_row(df, ['é—®é¢˜åˆ†ç±»', 'é¡¹ç›®åç§°'])

        if header_row is not None:
            df = standardize_column_names(df, header_row)
        else:
            print("âš ï¸ æœªæ‰¾åˆ°æ ‡å‡†è¡¨å¤´è¡Œï¼Œä½¿ç”¨åŸå§‹åˆ—å")

        print(f"ğŸ“‹ æœ€ç»ˆåˆ—å: {list(df.columns)}")

        # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
        if 'é—®é¢˜åˆ†ç±»' not in df.columns:
            print("âŒ è¡¨æ ¼ä¸­æ²¡æœ‰æ‰¾åˆ°'é—®é¢˜åˆ†ç±»'åˆ—")
            print(f"ğŸ“‹ å¯ç”¨åˆ—: {list(df.columns)}")
            return None

        return df

    except Exception as e:
        print(f"âŒ è¯»å–WPSæ•°æ®å¤±è´¥: {e}")
        return None

def filter_and_transform_data(df):
    """ç­›é€‰å’Œè½¬æ¢æ•°æ®"""
    print("ğŸ” ç­›é€‰è½¯ä»¶ç›¸å…³è®°å½•...")

    software_records = []
    field_mapping = CONFIG['fields']['mapping']

    # å…ˆæ˜¾ç¤ºæ‰€æœ‰é—®é¢˜åˆ†ç±»ç»Ÿè®¡
    category_counts = {}
    for idx, row in df.iterrows():
        problem_category = clean_string_value(row.get('é—®é¢˜åˆ†ç±»', ''))
        if problem_category:
            category_counts[problem_category] = category_counts.get(problem_category, 0) + 1

    print(f"ğŸ“Š é—®é¢˜åˆ†ç±»ç»Ÿè®¡: {category_counts}")

    for idx, row in df.iterrows():
        problem_category = clean_string_value(row.get('é—®é¢˜åˆ†ç±»', ''))
        project_name = clean_string_value(row.get('é¡¹ç›®åç§°', ''))

        # è°ƒè¯•è¾“å‡ºï¼šæ˜¾ç¤ºåŸå§‹æ•°æ®
        print(f"ğŸ” ç¬¬ {idx+1} è¡Œ: é¡¹ç›®='{project_name}', åˆ†ç±»='{problem_category}'")

        # è·³è¿‡ç©ºå€¼
        if is_empty_value(problem_category) and is_empty_value(project_name):
            print(f"   â­ï¸ è·³è¿‡ç©ºè¡Œ")
            continue

        # æ£€æŸ¥æ˜¯å¦ä¸ºè½¯ä»¶ç›¸å…³è®°å½•
        if is_valid_software_record(problem_category):
            try:
                # è½¬æ¢è®°å½•
                row_dict = row.to_dict() if hasattr(row, 'to_dict') else dict(row)
                record = transform_record(row_dict, field_mapping)

                print(f"   ğŸ”„ è½¬æ¢å: é¡¹ç›®='{record.get('project_name')}', åˆ†ç±»='{record.get('problem_category')}'")

                # éªŒè¯å¿…å¡«å­—æ®µ - ä½¿ç”¨è½¬æ¢åçš„å­—æ®µå
                missing_fields = []
                for required_field in CONFIG['fields']['required']:
                    # å°†ä¸­æ–‡å­—æ®µåæ˜ å°„åˆ°è‹±æ–‡å­—æ®µå
                    mapped_field = field_mapping.get(required_field, required_field)
                    if is_empty_value(record.get(mapped_field)):
                        missing_fields.append(required_field)

                if missing_fields:
                    print(f"   âš ï¸ ç¼ºå°‘å¿…å¡«å­—æ®µ: {missing_fields}")
                    continue

                software_records.append(record)
                print(f"   âœ… æ·»åŠ æˆåŠŸ: {record['issue_title']}")

            except Exception as e:
                print(f"   âŒ æ•°æ®è½¬æ¢å¤±è´¥: {e}")
                continue
        else:
            print(f"   âŒ ä¸æ˜¯è½¯ä»¶åˆ†ç±»ï¼Œè·³è¿‡")

    print(f"âœ… æ‰¾åˆ° {len(software_records)} æ¡æœ‰æ•ˆçš„è½¯ä»¶ç›¸å…³è®°å½•")
    return software_records

def display_upload_summary(api_data):
    """æ˜¾ç¤ºä¸Šä¼ æ‘˜è¦"""
    print("\n" + "="*60)
    print("ğŸ“¤ å³å°†ä¸Šä¼ çš„æ•°æ®æ‘˜è¦:")
    print("="*60)

    for i, record in enumerate(api_data[:10], 1):  # åªæ˜¾ç¤ºå‰10æ¡
        print(f"  {i}. {record['issue_title']}")
        print(f"     é—®é¢˜åˆ†ç±»: {record['problem_category']}")
        print(f"     çŠ¶æ€: {record['status']}")
        print(f"     è´£ä»»äºº: {record['responsible_person']}")
        print()

    if len(api_data) > 10:
        print(f"  ... è¿˜æœ‰ {len(api_data) - 10} æ¡è®°å½•")

    print(f"æ€»è®¡: {len(api_data)} æ¡è®°å½•")
    print("="*60)

# ==================== ä¸»å‡½æ•° ====================
def main():
    """ä¸»å‡½æ•°"""
    # è®¾ç½®æ—¥å¿—ï¼ˆä»…æ§åˆ¶å°è¾“å‡ºï¼‰
    logger = setup_logging()

    print("=" * 60)
    print("WPSè¡¨æ ¼æ•°æ®ä¸Šä¼ å·¥å…· - ä¼˜åŒ–ç‰ˆ")
    print("=" * 60)

    try:
        # 1. æµ‹è¯•æœåŠ¡å™¨è¿æ¥
        if not test_server_connection():
            print("âŒ æœåŠ¡å™¨è¿æ¥å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return False

        # 2. è¯»å–å’Œå¤„ç†WPSæ•°æ®
        df = read_and_process_wps_data()
        if df is None:
            print("âŒ æ•°æ®è¯»å–å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
            return False

        # 3. ç­›é€‰å’Œè½¬æ¢æ•°æ®
        valid_records = filter_and_transform_data(df)
        if not valid_records:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„è®°å½•")
            print(f"ğŸ’¡ æç¤º: å½“å‰æœç´¢ç›®æ ‡åˆ†ç±»ä¸º '{CONFIG['wps']['target_category']}'")
            return False

        # 4. æ˜¾ç¤ºä¸Šä¼ æ‘˜è¦
        display_upload_summary(valid_records)

        # 5. åˆ†æ‰¹ä¸Šä¼ æ•°æ®
        print("ğŸš€ å¼€å§‹ä¸Šä¼ æ•°æ®åˆ°æœåŠ¡å™¨...")
        success_count = upload_data_in_batches(valid_records)

        # 6. è¾“å‡ºç»“æœ
        success_ratio = success_count / len(valid_records)
        print(f"ğŸ“Š ä¸Šä¼ ç»“æœ: {success_count}/{len(valid_records)} æ¡è®°å½•æˆåŠŸ ({success_ratio:.1%})")

        return success_count > 0

    except Exception as e:
        print(f"âŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    print("å¼€å§‹æ‰§è¡ŒWPSè¡¨æ ¼æ•°æ®ä¸Šä¼ ...")
    success = main()

    if success:
        print("\nğŸ‰ ä¸Šä¼ å®Œæˆ! æ•°æ®å·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“")
    else:
        print("\nğŸ˜ ä¸Šä¼ å¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")

    print("\næŒ‰ä»»æ„é”®é€€å‡º...")
    try:
        input()
    except:
        pass
